---
title: "Reducing Commercial Aviation Fatalities"
author: "Zach Schuster"
date: "January 16, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(Kaggle.PKG.Aviation)
require(xgboost)
library(data.table)
```


Now we can process the entire training set

```{r}
readAndSplit(path = "data/train.csv",
             target_name = "event",
             name = "train")

# here we will randomly remove rows from class A as it is well over weight.
x_train[, event := y_train$event]

x_train = split(x_train, by = "event")
x_train = x_train[c("A", "B", "C", "D")]

rows = vapply(x_train, nrow, FUN.VALUE = integer(1))

# drastically undersample data to have equal balanced classes
x_train$A = x_train$A[sample.int(nrow(x_train$A), 1.2 * rows[["D"]])]
x_train$C = x_train$C[sample.int(nrow(x_train$C), 1.1 * rows[["D"]])]


x_train = data.table::rbindlist(x_train)

y_train = x_train[, "event"]
x_train[, event := NULL]

train = processTrainData(data = x_train,
                         y = y_train)

# save scaling params  and the columns to a .RData file
scale_means = train$scale_means
scale_sds = train$scale_sds
scale_cols = train$scale_cols

# save to disk for use in test data processing
save(scale_means, scale_sds, scale_cols, file = "data/scale_vars.RData")

# remove big files for space 
rm(x_train)
rm(y_train)

```


train xgboost model


```{r}

# define number of classes
nclass = 4

full_train = xgb.DMatrix(data.matrix(train$x_train),
                         label = multiCodeVars(train$y_train$event))

full_params = list(
  "objective" = "multi:softprob",
  "eval_metric" = "mlogloss",
  "num_class" = nclass,
  "eta" = c(.05),
  "gamma" = c(0),
  "max_depth" = c(5),
  "subsample" = .8,
  "colsample_bytree" = .8,
  "min_child_weight" = c(.6)
)
  
full_model = xgb.train(data = full_train,
                       nrounds = 1499,
                       params = full_params)

# save the model to disk for safety
xgboost::xgb.save(full_model, "models/full_xgb.model")

# remove unneeded objects for space
rm(list = ls()[!ls() %in% c("full_model", "full_params", "best_tune")])
gc(verbose = FALSE)
```

***

Now that the model has been trained, read in the test data and predict on test.

```{r process test data}
numrows = 17965143
chunk_size = 3e6
# test = fread("data/test.csv")
# 
test_preds = data.table::fread("data/test.csv",
                               select = "id")

# read in scale stuff
load("data/scale_vars.RData")

```

Write an lapply to process the test data in a chunk wise fashion

```{r}
testNames = names(data.table::fread("data/test.csv", nrows = 1))

test = lapply(1:ceiling(numrows/chunk_size), function(i) {
  
  test_part = data.table::fread("data/test.csv",
                                skip = 1 + ((i-1) * chunk_size),
                                nrows = chunk_size,
                                col.names = testNames)
  
  test_part = processTestData(test_part[, !"id"],
                              sc_means = scale_means,
                              sc_sds = scale_sds,
                              scale_cols = scale_cols)
  
  gc(verbose = FALSE)
  
  return(test_part)
})

test = data.table::rbindlist(test)

```


```{r }

# convert test to a xgb.Dmatrix
test = xgb.DMatrix(data = data.matrix(test))

# run garbage collection for space
gc(verbose = FALSE)
```

Finally predict on the test set and create predictions file. 

```{r}

# load in the trained model
# full_model = xgboost::xgb.load("models/full_xgb.model")

probs = formatXgbProbs(full_model, newdat = test)

test_results = setDT(cbind(test_preds, probs))
```

Write predictions to disk to be submitted

```{r}
fwrite(x = test_results, "data/test_results.csv")
```

