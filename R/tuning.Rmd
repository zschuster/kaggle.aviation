---
title: "Hyperparameter Tuning"
author: "Zach Schuster"
date: "January 29, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source("R/processing.R")
require(xgboost)
library(data.table)
```

First, read in the data

```{r}

readAndSplit(path = "data/train.csv",
             target_name = "event",
             name = "train")

# x_train = fread("data/train.csv")

```


for now, run cleaning on our training set so we can replicate it on our validation set.

We will split our training and test set it a way that we have the same proportions of reponse values in each set

```{r}
set.seed(0127)

train = setDT(cbind(x_train, y_train))

train = split(train, by = "event")

train_ind = lapply(train, function (dat) {
  sample.int(n = nrow(dat), size = .5 * nrow(dat), replace = FALSE)
})

train_small = data.table::rbindlist(
  mapply(function(dat, ind){
    return(dat[ind])
  }, dat = train, ind = train_ind, SIMPLIFY = FALSE)
)

test_small = data.table::rbindlist(
  mapply(function(dat, ind){
    return(dat[!ind])
  }, dat = train, ind = train_ind, SIMPLIFY = FALSE)
)

y_train_small = train_small[, "event"]
train_small = train_small[, !"event"]

y_test_small = test_small[, "event"]
test_small = test_small[, !"event"]

rm(x_train, y_train)

train = processTrainData(data = train_small,
                         y = y_train_small)

rm(train_small)
rm(y_train_small)

```


create xgb.DMatrix for training
```{r}
# for xgboost
# y_train_small = y_train[train_ind]
train_small = xgboost::xgb.DMatrix(data.matrix(train$x_train),
                                   label = multiCodeVars(train$y_train$event))

# create val set
val = processTestData(x_train[!train_ind], sc_means = train$scale_means,
                      sc_sds = train$scale_sds, scale_cols = train$scale_cols)
val = xgboost::xgb.DMatrix(data.matrix(val))

```


### Hyperparameter Tuning

create tuning grid

```{r, eval = FALSE}
# tuneGrid = expand.grid(
#   list(
#     eta = c(.2, .4),
#     gamma = c(0, .2),
#     max_depth = c(4, 9),
#     colsample_bytree = c(.9),
#     # alpha = c(0, .3),
#     min_child_weight = c(.8, 1.5)
#   )
# )

tuneGrid = expand.grid(
  list(
    eta = c(.8, 1.2),
    gamma = c(0),
    max_depth = c(9),
    colsample_bytree = c(.9),
    # alpha = c(0, .3),
    min_child_weight = c(.2)
  )
)

```

Run models for each parameter combination

```{r, eval = FALSE}
nclass = length(unique(y_train$event))

# create vector of class labels to calculate multi class log loss
val_labels = multiCodeVars(y_train[!train_ind]$event)


tuning = lapply(seq_len(nrow(tuneGrid)), function(i){
  
  # create list of parameters
  params = list(
    "eta" = tuneGrid$eta[i],
    "objective" = "multi:softprob",
    "eval_metric" = "mlogloss",
    "num_class" = nclass,
    "gamma" = tuneGrid$gamma[i],
    "max_depth" = tuneGrid$max_depth[i],
    # "subsample" = tuneGrid$subsample[i],
    "colsample_bytree" = tuneGrid$colsample_bytree[i],
    # "alpha" = tuneGrid$alpha[i],
    "min_child_weight" = tuneGrid$min_child_weight[i]
  )
  
  # train model
  model = xgb.train(
    data = train_small,
    params = params,
    nrounds = 10
    )
  
  # make predictions
  preds = formatXgbProbs(model, newdat = val)
  
  # status update
  cat(paste(i, "out of", nrow(tuneGrid), "models complete.\n"))

  # return results from this iteration of 
  return(
    data.table(
    eta = tuneGrid$eta[i],
    gamma = tuneGrid$gamma[i],
    max_depth = tuneGrid$max_depth[i],
    # subsample = tuneGrid$subsample[i],
    colsample_bytree = tuneGrid$colsample_bytree[i],
    # alpha = tuneGrid$alpha[i],
    min_child_weight = tuneGrid$min_child_weight[i],
    mLogLoss = MLmetrics::MultiLogLoss(y_pred = preds,
                        y_true = val_labels)
    )
  )
  
})

# combine list of results to find the best paramaters
results = data.table::rbindlist(tuning)

# order by performance
results = results[order(mLogLoss)]

# write results to disk
data.table::fwrite(results, "data/modeling_results.csv")

```


Create simple xgboost model

```{r}

nclass = 4

best_tune = fread("data/modeling_results.csv")
best_tune = best_tune[1, ]

train_params = list(
  "objective" = "multi:softprob",
  "eval_metric" = "mlogloss",
  "num_class" = nclass,
  "eta" = best_tune$eta,
  "gamma" = best_tune$gamma,
  "colsample_bytree" = best_tune$colsample_bytree,
  "max_depth" = best_tune$max_depth,
  "tree_method" = "exact"
  )


set.seed(0127)
xgb_train = xgboost::xgb.train(data = train_small,
                               nround = 25,
                               params = train_params)

```


Process our validation set

```{r}

val = x_train[!train_ind]

val = processTestData(val, lda_mod = train$lda_mod,
                      lda_cols = train$lda_cols)

val = xgboost::xgb.DMatrix(data.matrix(val))

# make predictions
preds = formatXgbProbs(xgb_train, newdat = val)

# check logloss
MLmetrics::MultiLogLoss(preds,
                        multiCodeVars(y_train[!train_ind]$event)
                        )

```

